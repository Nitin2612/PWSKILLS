{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QxnLB-RvBmzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is Logistic Regression, and how does it differ from Linear Regression?"
      ],
      "metadata": {
        "id": "i7iVRSRoBsiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression vs. Linear Regression\n",
        "\n",
        "**Logistic Regression** is a statistical model used primarily for **binary classification** tasks (though it can be extended to multiclass problems). Instead of predicting a continuous output, logistic regression predicts the **probability** that a given input belongs to a particular class. It uses the **sigmoid (logistic) function** to map any real-valued number into the range [0, 1], which can then be interpreted as a probability. The model is typically optimized using **maximum likelihood estimation**.\n",
        "\n",
        "**Linear Regression**, in contrast, is used for predicting **continuous outcomes**. It models the relationship between one or more independent variables and a continuous dependent variable by fitting a **linear equation**. The goal is to minimize the difference between the predicted values and the actual values, often using the **least squares method**.\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "- **Purpose:**\n",
        "  - *Logistic Regression:* Designed for classification problems where the outcome is categorical (e.g., 0/1, yes/no).\n",
        "  - *Linear Regression:* Designed for regression problems where the outcome is a continuous value.\n",
        "\n",
        "- **Output:**\n",
        "  - *Logistic Regression:* Outputs a probability (between 0 and 1) which is usually thresholded (e.g., at 0.5) to decide the class label.\n",
        "  - *Linear Regression:* Directly outputs a continuous numeric value.\n",
        "\n",
        "- **Transformation:**\n",
        "  - *Logistic Regression:* Uses the sigmoid function to squash the output of a linear equation into a [0, 1] range.\n",
        "  - *Linear Regression:* Directly uses the linear combination of the input features.\n",
        "\n",
        "- **Cost Function:**\n",
        "  - *Logistic Regression:* Uses a cost function based on **log loss (cross-entropy loss)** to measure the error.\n",
        "  - *Linear Regression:* Typically uses the **mean squared error (MSE)** as the cost function.\n",
        "\n",
        "### Summary\n",
        "\n",
        "In summary, while both logistic and linear regression involve forming a linear combination of input features, **logistic regression applies a non-linear sigmoid transformation to make predictions suitable for classification**, whereas **linear regression directly predicts a continuous outcome**. This fundamental difference shapes their usage, optimization strategies, and evaluation metrics.\n",
        "\n"
      ],
      "metadata": {
        "id": "6V-cSetrB83h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What is the mathematical equation of Logistic Regression?"
      ],
      "metadata": {
        "id": "ouhL6b5iCIzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Equation\n",
        "\n",
        "In logistic regression, we model the probability that a given input \\( \\mathbf{x} \\) belongs to a particular class (usually denoted as class 1) using the sigmoid function. The equation is:\n",
        "\n",
        "$$\n",
        "P(y=1 \\mid \\mathbf{x}) = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\quad \\text{where} \\quad z = \\mathbf{w}^T \\mathbf{x} + b\n",
        "$$\n",
        "\n",
        "- \\( \\mathbf{w} \\) is the vector of weights,\n",
        "- \\( \\mathbf{x} \\) is the vector of input features,\n",
        "- \\( b \\) is the bias term,\n",
        "- \\( \\sigma(z) \\) is the sigmoid function, which squashes the linear combination \\( z \\) into the range (0, 1).\n",
        "\n",
        "This formulation allows the model to output probabilities that can be thresholded (commonly at 0.5) to decide the predicted class.\n"
      ],
      "metadata": {
        "id": "Avhj8GrJCaAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. Why do we use the Sigmoid function in Logistic Regression?"
      ],
      "metadata": {
        "id": "g-a6BXOoCpCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **sigmoid function** is a crucial component of logistic regression for several key reasons:\n",
        "\n",
        "1. **Probability Mapping:**  \n",
        "   The sigmoid function converts any real-valued input into a value between 0 and 1. This property allows the output to be interpreted as a probability, which is essential for binary classification tasks where we want to estimate the likelihood that an instance belongs to a specific class.\n",
        "\n",
        "2. **Smooth and Differentiable:**  \n",
        "   The function is smooth and continuously differentiable. This makes it well-suited for optimization techniques like gradient descent, as the derivative of the sigmoid function is easy to compute and facilitates the training process.\n",
        "\n",
        "3. **Non-linear Transformation:**  \n",
        "   Although logistic regression forms a linear combination of inputs (i.e., \\( z = \\mathbf{w}^T \\mathbf{x} + b \\)), applying the sigmoid function introduces a non-linearity. This allows the model to handle more complex relationships in the data when estimating probabilities.\n",
        "\n",
        "4. **Interpretability:**  \n",
        "   Since the output of the sigmoid function represents a probability, it can be easily thresholded (commonly at 0.5) to make a clear decision regarding the class assignment. This adds to the interpretability of the modelâ€™s predictions.\n",
        "\n",
        "In summary, the sigmoid function is used in logistic regression to transform linear outputs into probability estimates that are both interpretable and optimized efficiently during training.\n"
      ],
      "metadata": {
        "id": "Gz9Rb2ZaC83H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. What is the cost function of Logistic Regression?"
      ],
      "metadata": {
        "id": "Fq_yyK3sDD3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Cost Function\n",
        "\n",
        "In logistic regression, the cost function is known as the **log loss** or **binary cross-entropy loss**. It measures the performance of the model by comparing the predicted probabilities with the actual labels. The cost function for \\( m \\) training examples is given by:\n",
        "\n",
        "$$\n",
        "J(w, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log\\left(\\hat{y}^{(i)}\\right) + \\left(1 - y^{(i)}\\right) \\log\\left(1 - \\hat{y}^{(i)}\\right) \\right]\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\( y^{(i)} \\) is the true label for the \\( i \\)-th example (either 0 or 1),\n",
        "- \\( \\hat{y}^{(i)} \\) is the predicted probability that the \\( i \\)-th example belongs to class 1, calculated using the sigmoid function:\n",
        "  \n",
        "  $$\n",
        "  \\hat{y}^{(i)} = \\sigma(z^{(i)}) = \\frac{1}{1 + e^{-z^{(i)}}} \\quad \\text{with} \\quad z^{(i)} = \\mathbf{w}^T \\mathbf{x}^{(i)} + b.\n",
        "  $$\n",
        "\n",
        "This cost function penalizes predictions that are far from the actual labels, and the training process involves finding the parameters \\( \\mathbf{w} \\) and \\( b \\) that minimize this cost.\n"
      ],
      "metadata": {
        "id": "ZhPgNmVtDLQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. What is Regularization in Logistic Regression? Why is it needed?"
      ],
      "metadata": {
        "id": "_5_po2SIDPE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization in Logistic Regression\n",
        "\n",
        "Regularization is a technique used to prevent overfitting by adding a penalty term to the cost function in logistic regression. This penalty discourages the model from fitting the noise in the training data by keeping the magnitude of the weights small.\n",
        "\n",
        "### Why is Regularization Needed?\n",
        "\n",
        "1. **Preventing Overfitting:**  \n",
        "   Without regularization, a model might learn not only the underlying patterns but also the noise in the training data. Regularization helps improve generalization to unseen data by penalizing overly complex models.\n",
        "\n",
        "2. **Controlling Model Complexity:**  \n",
        "   By adding a penalty for large weights, regularization encourages the model to be simpler. This is important because simpler models are usually more robust and less likely to overfit.\n",
        "\n",
        "3. **Improved Generalization:**  \n",
        "   A model that avoids overfitting is more likely to perform well on new, unseen data. Regularization thus directly contributes to the overall performance of the model.\n",
        "\n",
        "### Types of Regularization\n",
        "\n",
        "- **L2 Regularization (Ridge):**  \n",
        "  Adds a penalty equal to the square of the magnitude of the weights. The cost function becomes:\n",
        "\n",
        "  $$\n",
        "  J(w, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\Big[ y^{(i)} \\log\\left(\\hat{y}^{(i)}\\right) + \\big(1 - y^{(i)}\\big) \\log\\left(1 - \\hat{y}^{(i)}\\right) \\Big] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\n",
        "  $$\n",
        "\n",
        "  Here, \\( \\lambda \\) is the regularization parameter that controls the strength of the penalty.\n",
        "\n",
        "- **L1 Regularization (Lasso):**  \n",
        "  Adds a penalty equal to the absolute value of the weights. This form of regularization can drive some coefficients to zero, effectively performing feature selection.\n",
        "\n",
        "### Summary\n",
        "\n",
        "Regularization is critical in logistic regression to keep the model simple and prevent overfitting by penalizing large weights. This results in improved performance on new data and a more interpretable model.\n",
        "\n"
      ],
      "metadata": {
        "id": "vZPiGsUoDWVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. Explain the difference between Lasso, Ridge, and Elastic Net regression?"
      ],
      "metadata": {
        "id": "Ox90CpnRDgg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Differences between Lasso, Ridge, and Elastic Net Regression\n",
        "\n",
        "### Ridge Regression (L2 Regularization)\n",
        "- **Penalty Term:**  \n",
        "  Adds a penalty equal to the sum of the squared coefficients:\n",
        "  $$\n",
        "  \\text{Ridge Penalty} = \\frac{\\lambda}{2} \\sum_{j=1}^{n} w_j^2\n",
        "  $$\n",
        "- **Effect:**  \n",
        "  - Shrinks coefficients toward zero.\n",
        "  - Helps handle multicollinearity (i.e., when predictors are highly correlated).\n",
        "  - Typically, coefficients are reduced but rarely become exactly zero.\n",
        "\n",
        "### Lasso Regression (L1 Regularization)\n",
        "- **Penalty Term:**  \n",
        "  Adds a penalty equal to the sum of the absolute values of the coefficients:\n",
        "  $$\n",
        "  \\text{Lasso Penalty} = \\lambda \\sum_{j=1}^{n} |w_j|\n",
        "  $$\n",
        "- **Effect:**  \n",
        "  - Encourages sparsity in the model.\n",
        "  - Can drive some coefficients to exactly zero, effectively performing feature selection.\n",
        "  - Useful when you suspect that only a subset of the features is important.\n",
        "\n",
        "### Elastic Net Regression\n",
        "- **Penalty Term:**  \n",
        "  Combines both L1 and L2 penalties, often expressed with a mixing parameter \\( \\alpha \\) (with \\( 0 \\leq \\alpha \\leq 1 \\)):\n",
        "  $$\n",
        "  \\text{Elastic Net Penalty} = \\lambda \\left[ \\alpha \\sum_{j=1}^{n} |w_j| + \\frac{1-\\alpha}{2} \\sum_{j=1}^{n} w_j^2 \\right]\n",
        "  $$\n",
        "- **Effect:**  \n",
        "  - Balances the benefits of both Ridge and Lasso.\n",
        "  - Useful when there are many correlated features.\n",
        "  - Can both select important features (like Lasso) and stabilize the model (like Ridge).\n",
        "\n",
        "### Summary\n",
        "- **Ridge Regression:**  \n",
        "  Uses L2 regularization to shrink coefficients, which is helpful for multicollinearity but does not eliminate features.\n",
        "- **Lasso Regression:**  \n",
        "  Uses L1 regularization to enforce sparsity, effectively performing feature selection by zeroing out some coefficients.\n",
        "- **Elastic Net Regression:**  \n",
        "  Combines L1 and L2 penalties to benefit from both approaches, making it particularly useful when dealing with correlated predictors.\n"
      ],
      "metadata": {
        "id": "J5s_dXMODpub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7. When should we use Elastic Net instead of Lasso or Ridge?"
      ],
      "metadata": {
        "id": "tj-ZX_O8EY-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When to Use Elastic Net Instead of Lasso or Ridge\n",
        "\n",
        "Elastic Net regression combines the strengths of both Lasso (L1) and Ridge (L2) regularization. You might choose Elastic Net when:\n",
        "\n",
        "- **High Multicollinearity Among Predictors:**  \n",
        "  When predictors are highly correlated, Lasso tends to arbitrarily select one feature and discard others, while Ridge shrinks all coefficients but does not perform feature selection. Elastic Net, by combining both penalties, can select groups of correlated features together.\n",
        "\n",
        "- **Large Number of Predictors (p >> n):**  \n",
        "  In situations where the number of predictors exceeds the number of observations, Elastic Net's combination of L1 and L2 regularization helps control overfitting and improves prediction performance.\n",
        "\n",
        "- **Need for Feature Selection with Stability:**  \n",
        "  If you require feature selection (a benefit of Lasso) but want more stability in the selected features and coefficient estimates, Elastic Net provides a more balanced approach.\n",
        "\n",
        "- **Flexibility in Tuning the Bias-Variance Trade-off:**  \n",
        "  Elastic Net introduces a mixing parameter \\( \\alpha \\) (ranging between 0 and 1) that lets you control the balance between the L1 and L2 penalties. This flexibility can be crucial when you need to fine-tune the model to achieve optimal performance.\n",
        "\n",
        "In summary, Elastic Net is particularly useful when dealing with datasets that have many, possibly correlated, predictors, and when you want to combine the feature selection capability of Lasso with the coefficient shrinkage and grouping effect of Ridge.\n"
      ],
      "metadata": {
        "id": "RAxuqbA1EgrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8. What is the impact of the regularization parameter (Î») in Logistic Regression?"
      ],
      "metadata": {
        "id": "pa9LRQEZEmpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impact of the Regularization Parameter (\\(\\lambda\\)) in Logistic Regression\n",
        "\n",
        "The regularization parameter \\(\\lambda\\) is crucial in logistic regression as it controls the strength of the penalty applied to the model's weights. Here's how it impacts the model:\n",
        "\n",
        "1. **Weight Shrinkage:**  \n",
        "   - A **larger \\(\\lambda\\)** increases the penalty, causing the weights to shrink more towards zero.  \n",
        "   - A **smaller \\(\\lambda\\)** allows the weights to remain larger, enabling the model to capture more complex patterns but with a higher risk of overfitting.\n",
        "\n",
        "2. **Bias-Variance Trade-off:**  \n",
        "   - **High \\(\\lambda\\):** Results in a simpler model (higher bias) with reduced variance, which may generalize better on unseen data.  \n",
        "   - **Low \\(\\lambda\\):** Leads to a more complex model (lower bias) but may have higher variance, potentially overfitting the training data.\n",
        "\n",
        "3. **Generalization:**  \n",
        "   The correct tuning of \\(\\lambda\\) helps balance between fitting the training data well and maintaining good performance on new data.\n",
        "\n",
        "4. **Tuning:**  \n",
        "   \\(\\lambda\\) is typically tuned using cross-validation to find the optimal balance between bias and variance.\n",
        "\n",
        "For example, with L2 regularization, the logistic regression cost function becomes:\n",
        "\n",
        "$$\n",
        "J(w, b) = -\\frac{1}{m} \\sum_{i=1}^{m} \\Big[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\Big] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ti7jlokDEr8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q9. What are the key assumptions of Logistic Regression?"
      ],
      "metadata": {
        "id": "301TqFN5FXXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Assumptions of Logistic Regression\n",
        "\n",
        "Logistic Regression is widely used for binary classification, but it comes with several key assumptions:\n",
        "\n",
        "1. **Binary Outcome:**  \n",
        "   The dependent variable should be binary (e.g., 0/1, yes/no), since logistic regression models the probability of one of the two outcomes.\n",
        "\n",
        "2. **Linearity in the Logit:**  \n",
        "   There should be a linear relationship between the independent variables and the log odds (logit) of the dependent variable.  \n",
        "   Mathematically, if \\( p \\) is the probability of the positive class, then:\n",
        "\n",
        "   $$\n",
        "   \\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n\n",
        "   $$\n",
        "\n",
        "3. **Independence of Observations:**  \n",
        "   Each observation is assumed to be independent. Violations of this assumption (e.g., in time series or clustered data) can lead to biased estimates.\n",
        "\n",
        "4. **No (or Little) Multicollinearity:**  \n",
        "   The independent variables should not be highly correlated. High multicollinearity makes it difficult to distinguish individual effects and can destabilize coefficient estimates.\n",
        "\n",
        "5. **Large Sample Size:**  \n",
        "   Logistic regression typically uses maximum likelihood estimation, which generally requires a sufficiently large sample size to yield reliable estimates.\n",
        "\n",
        "6. **Correct Model Specification:**  \n",
        "   The model should include all relevant predictors and be properly specified in terms of the relationship between predictors and the outcome. Omitting important variables or including irrelevant ones can lead to misspecification.\n"
      ],
      "metadata": {
        "id": "iTuVL2mcFpcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q10. What are some alternatives to Logistic Regression for classification tasks?"
      ],
      "metadata": {
        "id": "KmCS_K8FGL4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternatives to Logistic Regression for Classification Tasks\n",
        "\n",
        "While Logistic Regression is popular for binary classification, several other methods may be more suitable depending on the problem characteristics and dataset. Here are some key alternatives:\n",
        "\n",
        "### 1. Decision Trees\n",
        "- **How It Works:**  \n",
        "  Splits data into subsets based on feature values, forming a tree structure where each leaf node represents a class.\n",
        "- **Pros:**  \n",
        "  Easy to interpret, handles both numerical and categorical features.\n",
        "- **Cons:**  \n",
        "  Prone to overfitting if not properly pruned.\n",
        "\n",
        "### 2. Random Forests\n",
        "- **How It Works:**  \n",
        "  An ensemble of decision trees where each tree votes on the class prediction.\n",
        "- **Pros:**  \n",
        "  Reduces overfitting compared to a single tree, robust to noise.\n",
        "- **Cons:**  \n",
        "  Less interpretable than individual decision trees.\n",
        "\n",
        "### 3. Support Vector Machines (SVM)\n",
        "- **How It Works:**  \n",
        "  Finds the hyperplane that best separates the classes in the feature space. With kernel functions, it can model non-linear boundaries.\n",
        "- **Pros:**  \n",
        "  Effective in high-dimensional spaces, versatile with different kernel choices.\n",
        "- **Cons:**  \n",
        "  Can be computationally intensive for large datasets.\n",
        "\n",
        "### 4. k-Nearest Neighbors (k-NN)\n",
        "- **How It Works:**  \n",
        "  Classifies a data point based on the majority class among its k closest neighbors in the feature space.\n",
        "- **Pros:**  \n",
        "  Simple and intuitive, no training phase required.\n",
        "- **Cons:**  \n",
        "  Prediction can be slow on large datasets, sensitive to the choice of k and feature scaling.\n",
        "\n",
        "### 5. Naive Bayes\n",
        "- **How It Works:**  \n",
        "  Uses Bayes' Theorem with the assumption of independence between predictors to compute class probabilities.\n",
        "- **Pros:**  \n",
        "  Fast, works well with high-dimensional data.\n",
        "- **Cons:**  \n",
        "  The independence assumption is often too simplistic.\n",
        "\n",
        "### 6. Neural Networks (Deep Learning)\n",
        "- **How It Works:**  \n",
        "  Comprises layers of interconnected neurons that learn complex, non-linear relationships through backpropagation.\n",
        "- **Pros:**  \n",
        "  Highly flexible, capable of modeling very complex patterns.\n",
        "- **Cons:**  \n",
        "  Requires large amounts of data and computational resources, less interpretable.\n",
        "\n",
        "### 7. Ensemble Methods (e.g., Gradient Boosting, AdaBoost)\n",
        "- **How It Works:**  \n",
        "  Combine multiple weak learners (e.g., decision trees) to create a strong predictive model. Boosting methods iteratively focus on misclassified examples.\n",
        "- **Pros:**  \n",
        "  Often provide superior predictive performance.\n",
        "- **Cons:**  \n",
        "  Increased model complexity and reduced interpretability.\n",
        "\n",
        "Each method has its own strengths and weaknesses. The best choice depends on factors such as the size and nature of the dataset, the need for model interpretability, and computational constraints.\n"
      ],
      "metadata": {
        "id": "e50WuBN5GV3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q11. What are Classification Evaluation Metrics?"
      ],
      "metadata": {
        "id": "_orHy8l8Gcr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Evaluation Metrics\n",
        "\n",
        "Classification metrics are used to evaluate the performance of a classification model. Here are some key metrics:\n",
        "\n",
        "### 1. Confusion Matrix\n",
        "A confusion matrix is a table that summarizes the performance of a classifier by comparing actual and predicted labels:\n",
        "- **True Positives (TP)**\n",
        "- **True Negatives (TN)**\n",
        "- **False Positives (FP)**\n",
        "- **False Negatives (FN)**\n",
        "\n",
        "### 2. Accuracy\n",
        "Accuracy is the proportion of correct predictions (both true positives and true negatives) out of all predictions:\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "\n",
        "### 3. Precision\n",
        "Precision (also called Positive Predictive Value) measures the proportion of positive predictions that are actually correct:\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "### 4. Recall (Sensitivity)\n",
        "Recall measures the proportion of actual positive cases that were correctly identified:\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "### 5. F1 Score\n",
        "The F1 score is the harmonic mean of precision and recall, providing a balance between the two:\n",
        "$$\n",
        "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "### 6. ROC Curve and AUC\n",
        "- **ROC Curve:** Plots the True Positive Rate (Recall) against the False Positive Rate for different threshold settings.\n",
        "- **AUC (Area Under the ROC Curve):** Quantifies the overall ability of the model to discriminate between positive and negative classes. A higher AUC indicates better performance.\n",
        "\n",
        "### 7. Log Loss (Cross-Entropy Loss)\n",
        "Log loss measures the performance of a classifier where the prediction is a probability between 0 and 1. Lower log loss values indicate better performance.\n",
        "\n",
        "\n",
        "Each metric provides unique insights into your model's performance. The best metric to use depends on your specific application and the balance between false positives and false negatives.\n"
      ],
      "metadata": {
        "id": "fQxCKO2wGjBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q12. How does class imbalance affect Logistic Regression?"
      ],
      "metadata": {
        "id": "8W-nQOH3G1-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class imbalance occurs when one class significantly outnumbers another. This can have several effects on a Logistic Regression model:\n",
        "\n",
        "1. **Bias Toward the Majority Class:**  \n",
        "   Logistic Regression minimizes a cost function that treats all misclassifications equally. When the dataset is imbalanced, the model may lean toward predicting the majority class to reduce overall error, often at the expense of correctly classifying the minority class.\n",
        "\n",
        "2. **Poor Minority Class Detection:**  \n",
        "   Even if the overall accuracy seems high, the model might perform poorly in identifying the minority class. This is critical in applications like fraud detection or medical diagnosis, where detecting the minority class is essential.\n",
        "\n",
        "3. **Skewed Probability Estimates:**  \n",
        "   The predicted probabilities may be biased toward the majority class, making it challenging to set a proper decision threshold. The standard threshold of 0.5 might not be appropriate when the classes are imbalanced.\n",
        "\n",
        "4. **Misleading Evaluation Metrics:**  \n",
        "   Accuracy can be misleading when one class dominates. Metrics such as precision, recall, F1 score, and ROC-AUC become more informative in assessing model performance under class imbalance.\n",
        "\n",
        "### Mitigation Strategies\n",
        "\n",
        "- **Resampling Techniques:**  \n",
        "  - **Over-sampling:** Increase the number of minority class examples (e.g., using SMOTE).  \n",
        "  - **Under-sampling:** Decrease the number of majority class examples.\n",
        "  \n",
        "- **Cost-Sensitive Learning:**  \n",
        "  Incorporate class weights in the loss function so that errors on the minority class incur a higher penalty.\n",
        "\n",
        "- **Threshold Adjustment:**  \n",
        "  Adjust the decision threshold from the default value (0.5) to optimize for better performance on the minority class based on evaluation metrics.\n",
        "\n",
        "Addressing class imbalance is crucial to ensure that Logistic Regression models provide reliable predictions for all classes.\n"
      ],
      "metadata": {
        "id": "VmdVSkJbG9r_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q13. What is Hyperparameter Tuning in Logistic Regression?"
      ],
      "metadata": {
        "id": "M7QZhSjxHLj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning in Logistic Regression\n",
        "\n",
        "Hyperparameter tuning refers to the process of selecting the optimal configuration for the parameters that are not learned directly from the data but set before the training process. In logistic regression, hyperparameter tuning primarily involves adjusting regularization parameters to improve model performance.\n",
        "\n",
        "## Key Hyperparameters\n",
        "\n",
        "1. **Regularization Strength (\\(\\lambda\\) or \\(C\\)):**  \n",
        "   - **\\(\\lambda\\):** In many formulations, \\(\\lambda\\) controls the amount of regularization. A larger \\(\\lambda\\) means stronger regularization, which penalizes large weights more heavily.  \n",
        "   - **\\(C\\):** In some libraries like Scikit-learn, the parameter \\(C\\) is used, where a smaller \\(C\\) corresponds to stronger regularization.\n",
        "\n",
        "2. **Type of Regularization:**  \n",
        "   - **L1 Regularization (Lasso):** Encourages sparsity by driving some coefficients to exactly zero, effectively performing feature selection.  \n",
        "   - **L2 Regularization (Ridge):** Penalizes the squared magnitude of coefficients, leading to smaller but non-zero values.  \n",
        "   - **Elastic Net:** A combination of both L1 and L2 penalties, controlled by a mixing parameter.\n",
        "\n",
        "## Methods for Hyperparameter Tuning\n",
        "\n",
        "- **Grid Search:**  \n",
        "  An exhaustive search over a specified parameter grid. Each combination is evaluated (often via cross-validation) to find the optimal settings.\n",
        "\n",
        "- **Random Search:**  \n",
        "  Randomly samples a subset of the hyperparameter space. It can be more efficient than grid search when dealing with many parameters or wide ranges.\n",
        "\n",
        "- **Automated Methods:**  \n",
        "  Techniques like Bayesian optimization, Hyperopt, or using tools such as Scikit-learn's `RandomizedSearchCV` and `GridSearchCV` can automate the tuning process.\n",
        "\n",
        "## Importance of Hyperparameter Tuning\n",
        "\n",
        "- **Improved Model Performance:**  \n",
        "  Properly tuned hyperparameters can enhance the modelâ€™s ability to generalize, leading to better performance on unseen data.\n",
        "\n",
        "- **Bias-Variance Trade-off:**  \n",
        "  Adjusting the regularization strength helps manage the trade-off between underfitting (high bias) and overfitting (high variance).\n",
        "\n",
        "- **Efficient Resource Use:**  \n",
        "  Tuning helps in finding a model that is neither too complex (which might overfit and be computationally expensive) nor too simple (which might underfit).\n",
        "\n",
        "In summary, hyperparameter tuning in logistic regression is essential for optimizing model performance and ensuring that the model is well-suited to the specific dataset and problem at hand.\n"
      ],
      "metadata": {
        "id": "_if7mwPpHS4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q14.  What are different solvers in Logistic Regression? Which one should be used?"
      ],
      "metadata": {
        "id": "ftj6xtOoHd7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solvers in Logistic Regression\n",
        "\n",
        "Logistic Regression involves optimizing a cost function, and different **solvers** are available to perform this optimization. Each solver uses a different algorithm, and the best choice depends on your dataset size, the type of regularization, and whether youâ€™re dealing with binary or multiclass classification. Here are the common solvers:\n",
        "\n",
        "### 1. **liblinear**\n",
        "- **Method:**  \n",
        "  Uses a coordinate descent algorithm.\n",
        "- **Pros:**  \n",
        "  - Well-suited for **small datasets**.\n",
        "  - Supports both **L1 and L2 regularization**.\n",
        "  - Works well for **binary classification**.\n",
        "- **Cons:**  \n",
        "  - Can be slower for large datasets.\n",
        "  - Not ideal for multiclass problems (it implements one-vs-rest for multiclass classification).\n",
        "\n",
        "### 2. **lbfgs**\n",
        "- **Method:**  \n",
        "  A quasi-Newton method that approximates the Hessian matrix.\n",
        "- **Pros:**  \n",
        "  - Efficient for **multiclass classification**.\n",
        "  - Works well with **large datasets**.\n",
        "  - Generally the **default solver** in many implementations (like Scikit-learn).\n",
        "- **Cons:**  \n",
        "  - Supports only **L2 regularization**.\n",
        "\n",
        "### 3. **newton-cg**\n",
        "- **Method:**  \n",
        "  Uses Newtonâ€™s method with a conjugate gradient approach to approximate the Hessian.\n",
        "- **Pros:**  \n",
        "  - Suitable for **large datasets**.\n",
        "  - Works well for **multiclass classification**.\n",
        "- **Cons:**  \n",
        "  - Typically supports only **L2 regularization**.\n",
        "\n",
        "### 4. **sag (Stochastic Average Gradient)**\n",
        "- **Method:**  \n",
        "  A variant of stochastic gradient descent optimized for large datasets.\n",
        "- **Pros:**  \n",
        "  - Scales well with **very large datasets**.\n",
        "  - Often faster than lbfgs for huge datasets.\n",
        "- **Cons:**  \n",
        "  - Generally limited to **L2 regularization**.\n",
        "  - May not perform as well on smaller datasets due to overhead.\n",
        "\n",
        "### 5. **saga**\n",
        "- **Method:**  \n",
        "  An extension of sag that can handle both L1 and L2 penalties.\n",
        "- **Pros:**  \n",
        "  - Suitable for **large datasets**.\n",
        "  - Supports **L1 regularization**, making it useful when you want feature selection.\n",
        "  - Works well with **sparse data**.\n",
        "- **Cons:**  \n",
        "  - Can be slightly more complex to tune.\n",
        "\n",
        "### Which One Should You Use?\n",
        "\n",
        "- **Small Datasets / Binary Classification:**  \n",
        "  Consider **liblinear** for its simplicity and support for both L1 and L2 regularization.\n",
        "\n",
        "- **Large Datasets / Multiclass Problems:**  \n",
        "  **lbfgs** or **newton-cg** are often the best choice, especially if you only need L2 regularization.  \n",
        "  For very large datasets, **sag** and **saga** may offer faster convergence.\n",
        "\n",
        "- **Need for L1 Regularization (Feature Selection):**  \n",
        "  Use **liblinear** (if the dataset is small) or **saga** (for larger datasets) to take advantage of L1's sparsity-inducing properties.\n",
        "\n",
        "In practice, the default choice in many libraries (like Scikit-learn) is **lbfgs** because of its robust performance on multiclass problems and larger datasets. However, itâ€™s often a good idea to experiment with different solvers and tune hyperparameters via cross-validation to determine the best option for your specific application.\n"
      ],
      "metadata": {
        "id": "0QnNqC0gHp32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q15.  How is Logistic Regression extended for multiclass classification?"
      ],
      "metadata": {
        "id": "Br7QzZqxH8fD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extending Logistic Regression for Multiclass Classification\n",
        "\n",
        "Logistic Regression was originally designed for binary classification. To extend it for multiclass problems (where there are more than two classes), there are two common approaches:\n",
        "\n",
        "### 1. One-vs-Rest (OvR) / One-vs-All\n",
        "- **Approach:**  \n",
        "  Train a separate binary logistic regression classifier for each class. For each classifier, the class in question is treated as the positive class, and all other classes are combined into a single negative class.\n",
        "- **Prediction:**  \n",
        "  Each classifier outputs a probability score. The final predicted class is the one with the highest probability.\n",
        "- **Advantages:**  \n",
        "  - Simple to implement.\n",
        "  - Can use binary logistic regression methods and solvers.\n",
        "- **Disadvantages:**  \n",
        "  - The classifiers are trained independently, which may lead to inconsistent probability estimates across classes.\n",
        "\n",
        "### 2. Multinomial Logistic Regression (Softmax Regression)\n",
        "- **Approach:**  \n",
        "  This method generalizes logistic regression by using the **softmax function** to directly model the probabilities of all classes in a single optimization problem.\n",
        "- **Mathematical Formulation:**  \n",
        "  For \\(K\\) classes and an input feature vector \\(\\mathbf{x}\\), the probability that the observation belongs to class \\(i\\) is given by:\n",
        "  $$\n",
        "  P(y = i \\mid \\mathbf{x}) = \\frac{e^{\\mathbf{w}_i^T \\mathbf{x} + b_i}}{\\sum_{j=1}^{K} e^{\\mathbf{w}_j^T \\mathbf{x} + b_j}}\n",
        "  $$\n",
        "  where:\n",
        "  - \\(\\mathbf{w}_i\\) and \\(b_i\\) are the weight vector and bias for class \\(i\\).\n",
        "- **Prediction:**  \n",
        "  The model predicts the class with the highest probability.\n",
        "- **Advantages:**  \n",
        "  - Provides a unified framework with consistent probability estimates across classes.\n",
        "  - Often yields better performance when class relationships are interdependent.\n",
        "- **Disadvantages:**  \n",
        "  - The optimization problem is generally more complex than in the binary case.\n",
        "\n",
        "### Which Approach to Choose?\n",
        "- **One-vs-Rest (OvR):**  \n",
        "  May be preferred when simplicity is key or when using solvers that are more efficient for binary problems.\n",
        "- **Multinomial Logistic Regression (Softmax):**  \n",
        "  Tends to be more robust and provides better-calibrated probabilities, especially when classes are not clearly separable.\n",
        "\n",
        "Both methods are widely used in practice, and the best choice often depends on the specifics of your dataset and computational resources.\n"
      ],
      "metadata": {
        "id": "9S4P5EewIDWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q16. What are the advantages and disadvantages of Logistic Regression?"
      ],
      "metadata": {
        "id": "LaPHgXQ3IQ5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advantages and Disadvantages of Logistic Regression\n",
        "\n",
        "## Advantages\n",
        "\n",
        "1. **Simplicity and Interpretability:**  \n",
        "   - **Easy to implement:** Logistic regression is straightforward to code and understand.  \n",
        "   - **Interpretable coefficients:** The model coefficients represent the change in the log odds of the outcome for a unit change in the predictor.\n",
        "\n",
        "2. **Computational Efficiency:**  \n",
        "   - Suitable for large datasets, as it converges relatively quickly with efficient optimization methods (e.g., lbfgs).\n",
        "\n",
        "3. **Probability Estimates:**  \n",
        "   - Provides calibrated probabilities that can be useful for decision-making beyond mere class predictions.\n",
        "\n",
        "4. **Regularization Capabilities:**  \n",
        "   - Incorporates regularization (L1, L2, or Elastic Net) to prevent overfitting, especially in high-dimensional settings.\n",
        "\n",
        "5. **Flexibility for Binary and Multiclass Problems:**  \n",
        "   - Easily extended to multiclass classification using techniques like one-vs-rest (OvR) or multinomial logistic regression (softmax).\n",
        "\n",
        "## Disadvantages\n",
        "\n",
        "1. **Assumption of Linearity in the Logit:**  \n",
        "   - Assumes a linear relationship between predictors and the log odds, which may not hold for complex datasets with non-linear relationships.\n",
        "\n",
        "2. **Sensitivity to Outliers:**  \n",
        "   - Outliers can have a significant impact on the decision boundary, potentially skewing results.\n",
        "\n",
        "3. **Difficulty with Highly Correlated Features:**  \n",
        "   - Multicollinearity among predictors can lead to unstable coefficient estimates unless addressed through regularization or feature selection.\n",
        "\n",
        "4. **Class Imbalance Challenges:**  \n",
        "   - In datasets with imbalanced classes, the model may be biased toward the majority class unless techniques like class weighting or resampling are applied.\n",
        "\n",
        "5. **Limited to Linearly Separable Problems:**  \n",
        "   - Logistic regression may underperform if the true decision boundary between classes is highly non-linear, requiring more complex models.\n"
      ],
      "metadata": {
        "id": "ucIYmZrHIdHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q17. What are some use cases of Logistic Regression?"
      ],
      "metadata": {
        "id": "ZekJn2vRIk-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Cases of Logistic Regression\n",
        "\n",
        "Logistic Regression is a versatile and widely used classification technique, particularly for binary outcomes. Here are some common applications:\n",
        "\n",
        "### 1. Medical Diagnosis\n",
        "- **Application:**  \n",
        "  Predicting whether a patient has a particular disease (e.g., diabetes, heart disease).\n",
        "- **Why Logistic Regression?**  \n",
        "  Its probabilistic outputs assist healthcare professionals in assessing risk and making informed decisions.\n",
        "\n",
        "### 2. Credit Scoring\n",
        "- **Application:**  \n",
        "  Evaluating the likelihood of a borrower defaulting on a loan.\n",
        "- **Why Logistic Regression?**  \n",
        "  The interpretability of the model allows lenders to understand the impact of various financial indicators on credit risk.\n",
        "\n",
        "### 3. Customer Churn Prediction\n",
        "- **Application:**  \n",
        "  Identifying customers who are likely to cancel a subscription or stop using a service.\n",
        "- **Why Logistic Regression?**  \n",
        "  By providing probability estimates, companies can target at-risk customers with retention strategies.\n",
        "\n",
        "### 4. Marketing Response Analysis\n",
        "- **Application:**  \n",
        "  Predicting whether a customer will respond to a marketing campaign.\n",
        "- **Why Logistic Regression?**  \n",
        "  It helps in segmenting customers based on their likelihood to engage or make a purchase, facilitating targeted marketing efforts.\n",
        "\n",
        "### 5. Fraud Detection\n",
        "- **Application:**  \n",
        "  Classifying transactions as fraudulent or legitimate.\n",
        "- **Why Logistic Regression?**  \n",
        "  Its efficiency and ease of interpretation make it suitable for real-time monitoring and quick decision-making.\n",
        "\n",
        "### 6. Spam Detection\n",
        "- **Application:**  \n",
        "  Determining whether an email is spam or not.\n",
        "- **Why Logistic Regression?**  \n",
        "  It can effectively handle high-dimensional data (e.g., text features) and deliver robust binary classifications.\n",
        "\n",
        "### 7. Social and Behavioral Research\n",
        "- **Application:**  \n",
        "  Analyzing survey data to predict binary outcomes (e.g., voter turnout, adoption of technology).\n",
        "- **Why Logistic Regression?**  \n",
        "  Its statistical foundations provide clear insights into the relationship between predictor variables and outcomes.\n",
        "\n",
        "Each of these use cases leverages the strengths of Logistic Regressionâ€”its simplicity, interpretability, and ability to generate probabilistic predictionsâ€”making it a valuable tool across various industries and research fields.\n"
      ],
      "metadata": {
        "id": "1HpWUHdDIvBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q18. What is the difference between Softmax Regression and Logistic Regression?"
      ],
      "metadata": {
        "id": "57_lRIzjJAI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Difference between Softmax Regression and Logistic Regression\n",
        "\n",
        "Both Logistic Regression and Softmax Regression are used for classification, but they differ primarily in the number of classes they handle and the functions they use to model probabilities.\n",
        "\n",
        "## Logistic Regression\n",
        "- **Purpose:**  \n",
        "  Designed for **binary classification** (two classes).\n",
        "  \n",
        "- **Probability Function:**  \n",
        "  Uses the **sigmoid function** to map a linear combination of inputs to a probability between 0 and 1:\n",
        "  $$\n",
        "  P(y=1 \\mid \\mathbf{x}) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + b)}}\n",
        "  $$\n",
        "  The probability of the other class is simply \\(P(y=0 \\mid \\mathbf{x}) = 1 - P(y=1 \\mid \\mathbf{x})\\).\n",
        "\n",
        "- **Output:**  \n",
        "  Provides a single probability value that can be thresholded (commonly at 0.5) to classify the input.\n",
        "\n",
        "## Softmax Regression\n",
        "- **Purpose:**  \n",
        "  An extension of Logistic Regression for **multiclass classification** (three or more classes).\n",
        "  \n",
        "- **Probability Function:**  \n",
        "  Uses the **softmax function** to convert a vector of scores (one for each class) into a probability distribution that sums to 1:\n",
        "  $$\n",
        "  P(y = i \\mid \\mathbf{x}) = \\frac{e^{\\mathbf{w}_i^T \\mathbf{x} + b_i}}{\\sum_{j=1}^{K} e^{\\mathbf{w}_j^T \\mathbf{x} + b_j}}, \\quad \\text{for } i = 1, \\dots, K\n",
        "  $$\n",
        "  where \\(K\\) is the number of classes.\n",
        "\n",
        "- **Output:**  \n",
        "  Produces a vector of probabilities for each class, with the predicted class being the one with the highest probability.\n",
        "\n",
        "## Key Differences\n",
        "- **Number of Classes:**\n",
        "  - **Logistic Regression:** Suitable for binary outcomes.\n",
        "  - **Softmax Regression:** Handles multiple classes simultaneously.\n",
        "  \n",
        "- **Function Used:**\n",
        "  - **Logistic Regression:** Sigmoid function.\n",
        "  - **Softmax Regression:** Softmax function, which normalizes outputs into a probability distribution.\n",
        "  \n",
        "- **Model Structure:**\n",
        "  - **Logistic Regression:** Single decision boundary separating two classes.\n",
        "  - **Softmax Regression:** Multiple decision boundaries to separate each class from the others.\n",
        "\n",
        "In summary, while Logistic Regression is ideal for problems with two possible outcomes, Softmax Regression generalizes the approach to work with multiclass classification by providing a full probability distribution over all classes.\n"
      ],
      "metadata": {
        "id": "TOXAmd41JUMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?"
      ],
      "metadata": {
        "id": "EFpGMUgOJg4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choosing Between One-vs-Rest (OvR) and Softmax for Multiclass Classification\n",
        "\n",
        "When extending logistic regression for multiclass problems, you have two popular approaches:\n",
        "\n",
        "## One-vs-Rest (OvR)\n",
        "- **Approach:**  \n",
        "  Train a separate binary logistic regression classifier for each class. Each classifier distinguishes one class from all others.\n",
        "- **Advantages:**  \n",
        "  - **Simplicity:** Uses standard binary logistic regression for each class.\n",
        "  - **Independent Training:** Each classifier can be tuned independently.\n",
        "  - **Flexibility:** Can perform well when classes are imbalanced.\n",
        "- **Disadvantages:**  \n",
        "  - **Inconsistent Probabilities:** The individual classifiers provide probabilities that are not naturally normalized (i.e., they may not sum to 1).\n",
        "  - **Suboptimal Boundaries:** Decision boundaries may not be optimal if classes overlap.\n",
        "- **When to Use:**  \n",
        "  - When you prefer simplicity and interpretability.\n",
        "  - When classes are relatively well-separated or when computational resources are limited.\n",
        "\n",
        "## Softmax Regression (Multinomial Logistic Regression)\n",
        "- **Approach:**  \n",
        "  Use a single model that computes scores for all classes and then applies the softmax function to convert these scores into a probability distribution.\n",
        "- **Advantages:**  \n",
        "  - **Unified Model:** Outputs a normalized probability distribution over all classes (the probabilities sum to 1).\n",
        "  - **Better Calibration:** Tends to provide more consistent probability estimates across classes.\n",
        "  - **Optimal for Interdependent Classes:** Can capture interactions between classes more effectively.\n",
        "- **Disadvantages:**  \n",
        "  - **Computational Complexity:** Optimizing a single multinomial objective can be more computationally intensive.\n",
        "  - **Interpretability:** The influence of individual predictors may be less straightforward compared to independent binary classifiers.\n",
        "- **When to Use:**  \n",
        "  - When classes are mutually exclusive and you require calibrated probability estimates.\n",
        "  - When you have sufficient computational resources to handle the more complex optimization.\n",
        "  - When the relationships between classes are interdependent.\n",
        "\n",
        "## How to Choose\n",
        "- **Dataset Characteristics:**  \n",
        "  - If your classes are simple and well-separated, OvR might be sufficient.\n",
        "  - For more complex or overlapping classes, Softmax can often provide better overall performance.\n",
        "- **Probability Calibration:**  \n",
        "  - If having a coherent probability distribution (i.e., probabilities summing to 1) is important, Softmax is preferable.\n",
        "- **Computational Considerations:**  \n",
        "  - OvR may be faster to train for small datasets or when leveraging highly optimized binary classifiers.\n",
        "  - Softmax is ideal for applications where a unified model is desired and computational resources allow for it.\n",
        "- **Empirical Evaluation:**  \n",
        "  - Often the best approach is to try both methods and compare their performance using cross-validation on your specific dataset.\n",
        "\n",
        "By weighing these factors, you can select the multiclass strategy that best fits your problem's requirements and data characteristics.\n"
      ],
      "metadata": {
        "id": "gTHmu61mJmbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q20. How do we interpret coefficients in Logistic Regression?"
      ],
      "metadata": {
        "id": "-niIWB-SKHnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpreting Coefficients in Logistic Regression\n",
        "\n",
        "In logistic regression, we model the log odds of the probability of an event occurring as a linear combination of the predictors:\n",
        "\n",
        "$$\n",
        "\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\( p \\) is the probability of the event (e.g., \\( y = 1 \\)).\n",
        "- \\( \\beta_0 \\) is the intercept.\n",
        "- \\( \\beta_i \\) (for \\( i = 1, 2, \\dots, k \\)) are the coefficients corresponding to each predictor \\( x_i \\).\n",
        "\n",
        "## How to Interpret the Coefficients\n",
        "\n",
        "### 1. Change in Log Odds\n",
        "- **Coefficient \\(\\beta_i\\):**  \n",
        "  Represents the change in the log odds of the outcome for a one-unit increase in \\( x_i \\), holding all other variables constant.\n",
        "- **Example:**  \n",
        "  If \\( \\beta_1 = 0.5 \\), a one-unit increase in \\( x_1 \\) increases the log odds of the event by 0.5.\n",
        "\n",
        "### 2. Odds Ratio\n",
        "- To convert the change in log odds to a more intuitive measure, we **exponentiate** the coefficient:\n",
        "  \n",
        "  $$\n",
        "  \\text{Odds Ratio} = e^{\\beta_i}\n",
        "  $$\n",
        "- **Interpretation:**\n",
        "  - If \\( e^{\\beta_i} > 1 \\): A one-unit increase in \\( x_i \\) increases the odds of the event.\n",
        "  - If \\( e^{\\beta_i} < 1 \\): A one-unit increase in \\( x_i \\) decreases the odds of the event.\n",
        "  - If \\( e^{\\beta_i} = 1 \\): There is no effect from \\( x_i \\).\n",
        "\n",
        "- **Example:**  \n",
        "  For \\( \\beta_1 = 0.7 \\):\n",
        "  - The odds ratio is \\( e^{0.7} \\approx 2.01 \\).\n",
        "  - This implies that a one-unit increase in \\( x_1 \\) approximately **doubles** the odds of the event occurring.\n",
        "\n",
        "### 3. Probability Interpretation\n",
        "- Although coefficients are interpreted in terms of log odds or odds ratios, the actual probability \\( p \\) is derived from the logistic (sigmoid) function:\n",
        "\n",
        "  $$\n",
        "  p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_k x_k)}}\n",
        "  $$\n",
        "\n",
        "- The relationship between predictors and the probability is **non-linear** due to the sigmoid transformation.\n",
        "\n",
        "### Considerations\n",
        "- **Scale and Units:**  \n",
        "  Ensure that the predictors are on a meaningful scale. For categorical predictors, coefficients represent differences relative to a reference category.\n",
        "- **Interaction and Nonlinear Effects:**  \n",
        "  When interaction terms or nonlinear transformations are included, the interpretation of individual coefficients becomes more complex.\n",
        "- **Context Matters:**  \n",
        "  Interpret the coefficients in the context of the domain, considering what a \"one-unit increase\" means for each variable.\n",
        "\n",
        "In summary, logistic regression coefficients inform us about the change in the log odds of an event per unit change in a predictor, and exponentiating these coefficients gives the odds ratios, which are often easier to interpret in practical terms.\n"
      ],
      "metadata": {
        "id": "qamHYMB_KMdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SoRpuK0HLQwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy?"
      ],
      "metadata": {
        "id": "QgoWginPLWFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouqHX6UdLd-M",
        "outputId": "11efe18f-9a60-410b-88ab-77f9292c2d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy?"
      ],
      "metadata": {
        "id": "ROrhJbzBMQmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OAAO_S8McsV",
        "outputId": "f8fc9896-e165-42cd-cbca-d8296b3df4d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients?"
      ],
      "metadata": {
        "id": "d1X_gSyYMjDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1uZbfp5MqPO",
        "outputId": "01a0f7f4-aee3-485e-90b2-c2bb2ac6300c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.956140350877193\n",
            "Coefficients: [[ 0.99123985  0.22585978 -0.36791774  0.02618353 -0.15473343 -0.23191094\n",
            "  -0.52309562 -0.27862312 -0.22387539 -0.0362507  -0.09516474  1.38669621\n",
            "  -0.1613307  -0.08921111 -0.02219822  0.04746336 -0.04182317 -0.03173778\n",
            "  -0.03381422  0.011614    0.09411541 -0.51440111 -0.0169633  -0.0165277\n",
            "  -0.3057193  -0.76483474 -1.41463479 -0.50351529 -0.73794024 -0.10032049]]\n",
            "Intercept: [29.4533058]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')?"
      ],
      "metadata": {
        "id": "pzi-1hA-M0Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNdLMxu_M7B2",
        "outputId": "5ab34c84-9be6-4ff4-acaf-2b5b11a91831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9736842105263158\n",
            "Coefficients: [[ 1.54887058e-02 -1.06195446e-03  7.70026171e-02  1.43956794e-02\n",
            "   0.00000000e+00 -3.22108805e-04 -6.58148061e-04 -1.74431074e-04\n",
            "   0.00000000e+00  0.00000000e+00  1.22588860e-05  4.47560881e-04\n",
            "  -1.37290833e-03 -2.29281756e-02  0.00000000e+00  0.00000000e+00\n",
            "  -2.30690439e-05  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   1.61473248e-02 -9.35733495e-03  6.31904961e-02 -2.72545199e-02\n",
            "   0.00000000e+00 -1.57262194e-03 -2.14254409e-03 -4.37649416e-04\n",
            "  -9.42097078e-05  0.00000000e+00]]\n",
            "Intercept: [0.00224565]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'?"
      ],
      "metadata": {
        "id": "NxwM2414NI1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noQ04DCWNTf3",
        "outputId": "e1067202-d0a9-4e01-871f-212d7c1fd2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Coefficients: [[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n",
            "Intercept: [ 0.2478905   0.86408083 -1.00411267]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy?"
      ],
      "metadata": {
        "id": "difWIBC0NaLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "log_reg = LogisticRegression(solver='liblinear', max_iter=10000)\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_params = grid_search.best_params_\n",
        "best_estimator = grid_search.best_estimator_\n",
        "y_pred = best_estimator.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "rT97lXkuNifI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "491ed798-88fd-4eff-e324-29fdcc47ca96"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 100, 'penalty': 'l1'}\n",
            "Test Accuracy: 0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy?"
      ],
      "metadata": {
        "id": "kaBBKHwsPmXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "print(\"Average Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "id": "sy_aZGzwPrRR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b61f1c3-3430-483d-9aec-9dfc7ac64fb5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.9508150908244062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy?"
      ],
      "metadata": {
        "id": "sGWUiUKjQDW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"/diabetes.csv\")\n",
        "# Print column names to verify the correct target column name\n",
        "print(\"Columns in CSV:\", df.columns)\n",
        "\n",
        "# Adjust the target column name if it's different (e.g., \"Outcome\")\n",
        "X = df.drop(\"Outcome\", axis=1)\n",
        "y = df[\"Outcome\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l-YOXwFQLIb",
        "outputId": "ea343ede-777f-48d2-b475-bf95fabe7047"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in CSV: Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
            "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
            "      dtype='object')\n",
            "Model Accuracy: 0.7467532467532467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy?"
      ],
      "metadata": {
        "id": "a-nXAHNxS3LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "log_reg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "param_distributions = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(log_reg, param_distributions, n_iter=10, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCppiksLWjBO",
        "outputId": "5fcfc6a8-6604-4807-f6d4-358434240187"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'liblinear', 'penalty': 'l2', 'C': 10}\n",
            "Test Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy?"
      ],
      "metadata": {
        "id": "lBlgWduxW5M-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "log_reg = LogisticRegression(max_iter=10000)\n",
        "ovo_model = OneVsOneClassifier(log_reg)\n",
        "ovo_model.fit(X_train, y_train)\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"OvO Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGIu6mysW-IX",
        "outputId": "016114fb-e455-4a4e-8ed8-d43a9d215a6c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvO Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification?"
      ],
      "metadata": {
        "id": "YhAiZUC9XA8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "1UpHQzW_XI4q",
        "outputId": "ca9584ff-484a-4077-fc92-1d57300ddf28"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score?"
      ],
      "metadata": {
        "id": "zKpY0D_fXULS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6BAeXNtXk8Y",
        "outputId": "ae07730d-5c13-4837-df81-46de96e9b4ba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9459459459459459\n",
            "Recall: 0.9859154929577465\n",
            "F1 Score: 0.9655172413793104\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94        43\n",
            "           1       0.95      0.99      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.96      0.95      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance?"
      ],
      "metadata": {
        "id": "1BEpr2_yXpvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_clusters_per_class=1, weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=10000, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa3XEB7tX1Nm",
        "outputId": "425971f4-9e65-4a67-84fc-73b11c7bc902"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98       186\n",
            "           1       0.67      1.00      0.80        14\n",
            "\n",
            "    accuracy                           0.96       200\n",
            "   macro avg       0.83      0.98      0.89       200\n",
            "weighted avg       0.98      0.96      0.97       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance?"
      ],
      "metadata": {
        "id": "qb379jORX5jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv(\"/content/titanic.csv\")\n",
        "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
        "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
        "df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
        "df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Did Not Survive\", \"Survived\"], yticklabels=[\"Did Not Survive\", \"Survived\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "AhZUDJRuX_j-",
        "outputId": "e4fc2e1f-0160-4a7a-f146-b4554f50aa78"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7877094972067039\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.84      0.82       105\n",
            "           1       0.76      0.72      0.74        74\n",
            "\n",
            "    accuracy                           0.79       179\n",
            "   macro avg       0.78      0.78      0.78       179\n",
            "weighted avg       0.79      0.79      0.79       179\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling?"
      ],
      "metadata": {
        "id": "ajb4e0Y9YnAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model_without_scaling = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "model_without_scaling.fit(X_train, y_train)\n",
        "y_pred_without_scaling = model_without_scaling.predict(X_test)\n",
        "accuracy_without_scaling = accuracy_score(y_test, y_pred_without_scaling)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "print(\"Accuracy without scaling:\", accuracy_without_scaling)\n",
        "print(\"Accuracy with scaling:\", accuracy_with_scaling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrOfkGm1YsJw",
        "outputId": "ba65e19d-35da-4ed0-8c0d-33cb7c3bd4cf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.956140350877193\n",
            "Accuracy with scaling: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score?"
      ],
      "metadata": {
        "id": "0vwfPdRAY05P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjcNu2XWY606",
        "outputId": "e6a7027f-f52b-4219-d910-3b320fc0271f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9977071732721914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy?"
      ],
      "metadata": {
        "id": "SIow5q9HZCrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(C=0.5, max_iter=10000, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28tZ3qp8ZMAs",
        "outputId": "119ddcdf-083c-49fa-f1da-b55f332a21fc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q18. Write a Python program to train Logistic Regression and identify important features based on model coefficients?"
      ],
      "metadata": {
        "id": "KXh3SHG9ZPJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "coefs = model.coef_[0]\n",
        "df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefs})\n",
        "df['Absolute Coefficient'] = df['Coefficient'].abs()\n",
        "df_sorted = df.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "print(\"Important Features Based on Model Coefficients:\")\n",
        "print(df_sorted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHmGuoClZXW_",
        "outputId": "b8a6a3bd-070c-4df0-be1b-1e2bfe8d133b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important Features Based on Model Coefficients:\n",
            "                    Feature  Coefficient  Absolute Coefficient\n",
            "0               mean radius     2.132484              2.132484\n",
            "26          worst concavity    -1.617969              1.617969\n",
            "11            texture error     1.442984              1.442984\n",
            "20             worst radius     1.232150              1.232150\n",
            "25        worst compactness    -1.208985              1.208985\n",
            "28           worst symmetry    -0.742764              0.742764\n",
            "6            mean concavity    -0.651940              0.651940\n",
            "27     worst concave points    -0.615251              0.615251\n",
            "5          mean compactness    -0.415569              0.415569\n",
            "21            worst texture    -0.404581              0.404581\n",
            "7       mean concave points    -0.344456              0.344456\n",
            "12          perimeter error    -0.303857              0.303857\n",
            "24         worst smoothness    -0.262631              0.262631\n",
            "8             mean symmetry    -0.207613              0.207613\n",
            "1              mean texture     0.152772              0.152772\n",
            "2            mean perimeter    -0.145091              0.145091\n",
            "4           mean smoothness    -0.142636              0.142636\n",
            "29  worst fractal dimension    -0.116960              0.116960\n",
            "13               area error    -0.072569              0.072569\n",
            "10             radius error    -0.050034              0.050034\n",
            "16          concavity error    -0.044886              0.044886\n",
            "18           symmetry error    -0.041752              0.041752\n",
            "17     concave points error    -0.037719              0.037719\n",
            "22          worst perimeter    -0.036209              0.036209\n",
            "9    mean fractal dimension    -0.029774              0.029774\n",
            "23               worst area    -0.027087              0.027087\n",
            "14         smoothness error    -0.016159              0.016159\n",
            "19  fractal dimension error     0.005613              0.005613\n",
            "15        compactness error    -0.001907              0.001907\n",
            "3                 mean area    -0.000829              0.000829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q19. Write a Python program to train Logistic Regression and evaluate its performance using Cohenâ€™s Kappa Score?"
      ],
      "metadata": {
        "id": "KisjXSDGZcdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen's Kappa Score:\", kappa)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6baoamOAZkOk",
        "outputId": "b41e937b-da22-40ea-9589-194b6449c976"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.9053470607771504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification?"
      ],
      "metadata": {
        "id": "Bd3AyLRIZor-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "y_scores = model.decision_function(X_test)\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "ap_score = average_precision_score(y_test, y_scores)\n",
        "print(\"Average Precision Score:\", ap_score)\n",
        "plt.plot(recall, precision, marker='.', label='Logistic Regression')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "GNH553O_ZyZE",
        "outputId": "51906a11-2e7e-436a-96a8-677b7b06e5e1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Precision Score: 0.9986451224185282\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy?"
      ],
      "metadata": {
        "id": "IPi7QfrpZ2hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "results = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=10000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results[solver] = acc\n",
        "\n",
        "print(\"Accuracy for different solvers:\")\n",
        "for solver, acc in results.items():\n",
        "    print(f\"{solver}: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2KME04AZ-q0",
        "outputId": "7c4d548f-4c04-49e4-f770-ecb69f230512"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for different solvers:\n",
            "liblinear: 0.9561\n",
            "saga: 0.9737\n",
            "lbfgs: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)?"
      ],
      "metadata": {
        "id": "NCEI1VZ9aP1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(\"Matthews Correlation Coefficient:\", mcc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX3RVG5OaWy-",
        "outputId": "63cdcf8d-1c81-4678-c0e1-aa4d355ec9ac"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient: 0.9068106119605033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling?"
      ],
      "metadata": {
        "id": "1uONuI1kagNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model_raw = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "model_scaled = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy on raw data:\", accuracy_raw)\n",
        "print(\"Accuracy on standardized data:\", accuracy_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvLQdE3famZj",
        "outputId": "bd6b72a9-6e5c-4149-fbbc-cce223e40c28"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 0.956140350877193\n",
            "Accuracy on standardized data: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation?"
      ],
      "metadata": {
        "id": "dv_vuIv4auyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=10000, solver='liblinear'),\n",
        "                           param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best C:\", grid_search.best_params_['C'])\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpaTFLhWa1k5",
        "outputId": "6fc08fee-2d3a-4bee-afd2-44174b6316fd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C: 10\n",
            "Test Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions?"
      ],
      "metadata": {
        "id": "TBzIA-eMbDRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=10000, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "joblib.dump(model, 'logistic_regression_model.joblib')\n",
        "loaded_model = joblib.load('logistic_regression_model.joblib')\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2bK_pRGbH1Z",
        "outputId": "f238ac7c-d590-43e8-9a16-e8c6023c50bb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    }
  ]
}